{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要探索如何使用深度学习框架 Gluon 实现**线性回归**模型？并且以 Kaggle 上数据集 [USA_Housing](https://www.kaggle.com/vedavyasv/usa-housing) 做线性回归任务来预测房价。\n",
    "\n",
    "回归任务，scikit-learn 亦可以实现，具体操作可以查看 [线性回归模型的原理与 scikit-learn 实现](https://www.jianshu.com/p/a65c3965e290)。\n",
    "\n",
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:15:59.564954Z",
     "start_time": "2018-12-03T13:15:59.177972Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "name = '../dataset/USA_Housing.csv'\n",
    "dataset = pd.read_csv(name)\n",
    "\n",
    "train = dataset.iloc[:3000,:]\n",
    "test = dataset.iloc[3000:,:]\n",
    "\n",
    "features_column = [\n",
    "    name for name in dataset.columns if name not in ['Price', 'Address']\n",
    "]\n",
    "label_column = ['Price']\n",
    "\n",
    "x_train = train[features_column]\n",
    "y_train = train[label_column]\n",
    "x_test = test[features_column]\n",
    "y_test = test[label_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据标准化\n",
    "\n",
    "线性回归模型就是单层神经网络，在神经网络的训练中，需要将数据进行标准化处理，使得数据的尺度统一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:16:00.478988Z",
     "start_time": "2018-12-03T13:16:00.465966Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "x_train_s = scale(x_train)\n",
    "x_test_s = scale(x_test)\n",
    "y_train_s = scale(y_train)\n",
    "y_test_s = scale(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:56:19.857057Z",
     "start_time": "2018-12-03T12:56:19.843055Z"
    }
   },
   "source": [
    "为了更红的管理数据集我们先定义一个针对数据集处理的统一 API：`Loader`。为了和不同的深度学习框架进行接洽，`Loader` 被限制为输出 Numpy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:16:03.819983Z",
     "start_time": "2018-12-03T13:16:03.811949Z"
    }
   },
   "outputs": [],
   "source": [
    "class Loader(dict):\n",
    "    \"\"\"\n",
    "    方法\n",
    "    ========\n",
    "    L 为该类的实例\n",
    "    len(L)::返回样本数目\n",
    "    iter(L)::即为数据迭代器\n",
    "\n",
    "    Return\n",
    "    ========\n",
    "    可迭代对象（numpy 对象）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, X, Y=None, shuffle=True, name=None):\n",
    "        '''\n",
    "        X, Y 均为类 numpy, 可以是 HDF5\n",
    "        '''\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        self.X = np.asanyarray(X[:])\n",
    "        if Y is None:\n",
    "            # print('不存在标签！')\n",
    "            self.Y = None\n",
    "        else:\n",
    "            self.Y = np.asanyarray(Y[:])\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.nrows = self.X.shape[0]\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx = np.arange(self.nrows)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "\n",
    "        for k in range(0, self.nrows, self.batch_size):\n",
    "            K = idx[k:min(k + self.batch_size, self.nrows)]\n",
    "            if self.Y is None:\n",
    "                yield np.take(self.X, K, 0)\n",
    "            else:\n",
    "                yield np.take(self.X, K, 0), np.take(self.Y, K, 0)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们便可以获得满足深度学习框架的训练数据集和测试数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:26:51.782283Z",
     "start_time": "2018-12-03T13:26:51.778273Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "trainset = Loader(batch_size, x_train_s, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:31:16.850051Z",
     "start_time": "2018-12-03T13:31:14.836038Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.420815\n",
      "epoch 2, loss: 0.335122\n",
      "epoch 3, loss: 0.273987\n",
      "epoch 4, loss: 0.230418\n",
      "epoch 5, loss: 0.197117\n",
      "epoch 6, loss: 0.171459\n",
      "epoch 7, loss: 0.151247\n",
      "epoch 8, loss: 0.134480\n",
      "epoch 9, loss: 0.119742\n",
      "epoch 10, loss: 0.106986\n",
      "epoch 11, loss: 0.095315\n",
      "epoch 12, loss: 0.084960\n",
      "epoch 13, loss: 0.075951\n",
      "epoch 14, loss: 0.068014\n",
      "epoch 15, loss: 0.061575\n",
      "epoch 16, loss: 0.056335\n",
      "epoch 17, loss: 0.052137\n",
      "epoch 18, loss: 0.049383\n",
      "epoch 19, loss: 0.046681\n",
      "epoch 20, loss: 0.044657\n",
      "epoch 21, loss: 0.043820\n",
      "epoch 22, loss: 0.042778\n",
      "epoch 23, loss: 0.042254\n",
      "epoch 24, loss: 0.042134\n",
      "epoch 25, loss: 0.041724\n",
      "epoch 26, loss: 0.041711\n",
      "epoch 27, loss: 0.041550\n",
      "epoch 28, loss: 0.041608\n",
      "epoch 29, loss: 0.041457\n",
      "epoch 30, loss: 0.041672\n",
      "epoch 31, loss: 0.041680\n",
      "epoch 32, loss: 0.041552\n",
      "epoch 33, loss: 0.041596\n",
      "epoch 34, loss: 0.041865\n",
      "epoch 35, loss: 0.041722\n",
      "epoch 36, loss: 0.041452\n",
      "epoch 37, loss: 0.041614\n",
      "epoch 38, loss: 0.041962\n",
      "epoch 39, loss: 0.041465\n",
      "epoch 40, loss: 0.041769\n",
      "epoch 41, loss: 0.041390\n",
      "epoch 42, loss: 0.041540\n",
      "epoch 43, loss: 0.041762\n",
      "epoch 44, loss: 0.041454\n",
      "epoch 45, loss: 0.041711\n",
      "epoch 46, loss: 0.041645\n",
      "epoch 47, loss: 0.041483\n",
      "epoch 48, loss: 0.041548\n",
      "epoch 49, loss: 0.041688\n",
      "epoch 50, loss: 0.041770\n",
      "epoch 51, loss: 0.041764\n",
      "epoch 52, loss: 0.041410\n",
      "epoch 53, loss: 0.041811\n",
      "epoch 54, loss: 0.041365\n",
      "epoch 55, loss: 0.041610\n",
      "epoch 56, loss: 0.041790\n",
      "epoch 57, loss: 0.041329\n",
      "epoch 58, loss: 0.041803\n",
      "epoch 59, loss: 0.041628\n",
      "epoch 60, loss: 0.041491\n",
      "epoch 61, loss: 0.041454\n",
      "epoch 62, loss: 0.041726\n",
      "epoch 63, loss: 0.041369\n",
      "epoch 64, loss: 0.041451\n",
      "epoch 65, loss: 0.041493\n",
      "epoch 66, loss: 0.041322\n",
      "epoch 67, loss: 0.041802\n",
      "epoch 68, loss: 0.041482\n",
      "epoch 69, loss: 0.041294\n",
      "epoch 70, loss: 0.041567\n",
      "epoch 71, loss: 0.041624\n",
      "epoch 72, loss: 0.041588\n",
      "epoch 73, loss: 0.041211\n",
      "epoch 74, loss: 0.041704\n",
      "epoch 75, loss: 0.041462\n",
      "epoch 76, loss: 0.041459\n",
      "epoch 77, loss: 0.041398\n",
      "epoch 78, loss: 0.041853\n",
      "epoch 79, loss: 0.041366\n",
      "epoch 80, loss: 0.041663\n",
      "epoch 81, loss: 0.041810\n",
      "epoch 82, loss: 0.041263\n",
      "epoch 83, loss: 0.041584\n",
      "epoch 84, loss: 0.041663\n",
      "epoch 85, loss: 0.041361\n",
      "epoch 86, loss: 0.041498\n",
      "epoch 87, loss: 0.041386\n",
      "epoch 88, loss: 0.041287\n",
      "epoch 89, loss: 0.041486\n",
      "epoch 90, loss: 0.041396\n",
      "epoch 91, loss: 0.041667\n",
      "epoch 92, loss: 0.041652\n",
      "epoch 93, loss: 0.041454\n",
      "epoch 94, loss: 0.041437\n",
      "epoch 95, loss: 0.041703\n",
      "epoch 96, loss: 0.041224\n",
      "epoch 97, loss: 0.041604\n",
      "epoch 98, loss: 0.041512\n",
      "epoch 99, loss: 0.041346\n",
      "epoch 100, loss: 0.041565\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon\n",
    "\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "loss = gloss.L2Loss()  # 平方损失又称 L2 范数损失。\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adadelta', {'rho': 0.9})\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in trainset:\n",
    "        X = nd.array(X)\n",
    "        y = nd.array(y)\n",
    "        with autograd.record():\n",
    "            #  由于预测值很大，我们可以将其转换为小尺度的数据\n",
    "            out = net(X)\n",
    "            out = nd.relu(out)\n",
    "            l = loss(out, y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(nd.array(x_test_s)), nd.array(y_test_s))\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:31:19.553495Z",
     "start_time": "2018-12-03T13:31:19.544512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168694624481616"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "out = net(nd.array(x_test_s)).asnumpy()\n",
    "r2_score(y_test_s, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
