## 深度学习基础

设有数据集 $\{ x^{(1)}, x^{(2)}, \ldots, x^{(m)} \}$，对于每一个样本 $x^{(i)} \in \mathbb{R}^n$，令

$$
\begin{cases}
X = \begin{pmatrix} x^{(1)}\\ x^{(2)}\\ \vdots\\ x^{(m)} \end{pmatrix}\\
Y = \begin{pmatrix} y^{(1)}\\ y^{(2)}\\ \vdots\\ y^{(m)} \end{pmatrix}\\
\end{cases}
$$

称 $X$ 为数据集 $\{(x^{(i)}, y^{(i)})\}_{i=1}^m$ 的**设计矩阵**。其中，$y^{(i)}$ 为 $x^{(i)}$ 对应的标签。

**注意**：

- 对于回归任务，$y^{(i)} \in \mathbb{R}$；
- 对于分类任务，$y^{(i)} \in \mathbb{R}^c$，其中 $c$ 为该数据集拥有的类别个数。

### 1  线性回归模型

我们先考虑一个样本 $x^{(i)}$，$w \in \mathbb{R}^n, b \in \mathbb{R}$，有

$$
\begin{aligned}
&\hat{y}^{(i)} = x^{(i)} w + b\\
&\ell_i = \frac{1}{2} (y^{(i)} -\hat{y}^{(i)})^2
\end{aligned}
$$

再考虑所有样本，有

$$
\begin{aligned}
&\hat{Y} = Xw + b \cdot \mathbb{1}\\
&\ell = \frac{1}{2m} \sum_{i=1}^m \ell_i = \frac{1}{2m} ||Xw + b \cdot \mathbb{1} - Y||_2^2
\end{aligned}
$$

下面我们来看看如何更新参数的？

#### 1.1  梯度下降

我们先求下梯度：

$$
\begin{aligned}
&\nabla_{w} = \frac{\partial \ell}{\partial w} = \frac{1}{m} X^T (Xw + b \cdot \mathbb{1} - Y)\\
&\nabla_b =  \frac{\partial \ell}{\partial b} = \frac{1}{m} \mathbb{1}^T \cdot (Xw + b \cdot \mathbb{1} - Y)
\end{aligned}
$$

再更新参数：

$$
\begin{aligned}
&w = w - \alpha \nabla w\\
&b = b - \alpha \nabla b
\end{aligned}
$$

其中，$\alpha$ 被称为**学习率**或**步长**。

#### 1.2  最小二乘法

令 $\theta = \begin{pmatrix} w \\ b  \end{pmatrix}$，$\overline{X} = \begin{pmatrix} X & 1  \end{pmatrix} = \begin{pmatrix} \overline{x}^{(1)} \\ \overline{x}^{(2)} \\ \vdots \\ \overline{x}^{(m)} \end{pmatrix}$，则

$$
\ell = \frac{1}{m} ||\overline{X} \theta - Y||_F^2
$$

由 $\frac{\partial \ell}{\partial \theta} = 0$ 可得最小二乘解

$$
\theta^{*} = (\overline{X}^T \overline{X})^{\dagger} Y
$$

[线性回归模型的原理与 scikit-learn 实现](https://www.jianshu.com/p/a65c3965e290)

### 2  softmax 回归

softmax Regression 是解决多分类任务的模型。此时 $y^{(i)} \in \{1, 2, \ldots, c\}$。

设每个样本的条件概率估计为

$$
\mathbb{\hat{y}^{(i)}} = h_{\theta}(\overline{x}^{i}) = \begin{bmatrix}
P(y^{(i)} = 1| \overline{x}^{(i)}; \theta) \\
P(y^{(i)} = 2| \overline{x}^{(i)}; \theta)\\
\vdots\\
P(y^{(i)} = c| \overline{x}^{(i)}; \theta)\\
\end{bmatrix} = \frac{1}{\sum_{j=1}^k e^{\theta_j^T \overline{x}^{(i)}}}
\begin{bmatrix}
 e^{\theta_1^T \overline{x}^{(i)}} \\
 e^{\theta_2^T \overline{x}^{(i)}} \\
\vdots\\
 e^{\theta_c^T \overline{x}^{(i)}} \\
\end{bmatrix}
$$

其 python 代码实现很简单：

```python
def softmax(X):
    exp = np.exp(X)
    # 假设 exp 是矩阵，这里对行进行求和，并要求保留 axis 1，
    # 就是返回 (nrows, 1) 形状的矩阵
    partition = exp.sum(axis=1, keepdims=True)
    return exp / partition
```

softmax Regression 模型的损失函数不适合使用 $\ell_2$ 损失函数，此时使用**交叉熵**损失函数。将 $y^{(i)}$ 转换为 one-hot 编码形式：

$$
y^{(i)} = t \Leftrightarrow y^{(i)}_k = \begin{cases}
1 & k = t\\
0 & k \neq t
 \end{cases}
$$

其中，$k,t \in \{1, 2, \ldots, c\}$。这样，${\boldsymbol y^{(i)}} = (y^{(i)}_1, y^{(i)}_2, \dots, y^{(i)}_c)$ 便可表示 $y^{(i)}$ 的概率形式。这样，我们便可定义两个概率分布的“距离”：

$$
H({\boldsymbol y^{(i)}}, {\boldsymbol y^{(i)}}) = - \sum_{j=1}^c y_j^{(i)} \log \hat{y}_j^{(i)}
$$

由于向量 ${\mathbb y^{(i)}}$ 中元素的特性，当一个样本中仅仅包含一个对象时上面的交叉熵我们可以化简为

$$
H({\boldsymbol y^{(i)}}, \boldsymbol{\hat{y}^{(i)}}) = - \log \hat{y}_t^{(i)}
$$

其中，$t$ 表示 ${\mathbb y^{(i)}}$ 中 $t$ 位置元素为 $1$，下面我们使用 $y^{(i)}$ 代替 $t$。

对于所有样本，交叉熵损失函数定义为

$$\ell(\boldsymbol{\Theta}) = \frac{1}{m} \sum_{i=1}^m H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),$$

其中$\boldsymbol{\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/m)  \sum_{i=1}^m \log \hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\ell(\boldsymbol{\Theta})$等价于最大化$\exp(-m\ell(\boldsymbol{\Theta}))=\prod_{i=1}^m \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。
