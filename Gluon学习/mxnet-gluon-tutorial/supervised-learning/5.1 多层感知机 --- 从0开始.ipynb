{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机 --- 从0开始\n",
    "\n",
    "前面我们介绍了包括线性回归和多类逻辑回归的数个模型，它们的一个共同点是全是只含有一个输入层，一个输出层。这一节我们将介绍多层神经网络，就是包含至少一个隐含层的网络。\n",
    "\n",
    "## 数据获取\n",
    "\n",
    "我们继续使用 FashionMNIST 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "root = 'E:/Data/MXNet/fashion_mnist'\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype('float32')/255, label.astype('float32')\n",
    "mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform, root= root)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform, root= root)\n",
    "\n",
    "train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机\n",
    "\n",
    "多层感知机与前面介绍的*多类逻辑回归*非常类似，主要的区别是我们在输入层和输出层之间插入了一个到多个隐含层。\n",
    "\n",
    "这里我们定义一个只有一个隐含层的模型，这个隐含层输出 $256$ 个节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden = 256\n",
    "weight_scale = .01\n",
    "\n",
    "W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "b1 = nd.zeros(num_hidden)\n",
    "\n",
    "W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "b2 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "\n",
    "如果我们就用线性操作符来构造多层神经网络，那么整个模型仍然只是一个线性函数。这是因为\n",
    "\n",
    "$$\\hat{y} = X \\cdot W_1 \\cdot W_2 = X \\cdot W_3 $$\n",
    "\n",
    "这里$W_3 = W_1 \\cdot W_2$。为了让我们的模型可以拟合非线性函数，我们需要在层之间插入非线性的激活函数。这里我们使用ReLU\n",
    "\n",
    "$$\\textrm{rel}u(x)=\\max(x, 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们的模型就是将层（全连接）和激活函数（Relu）串起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    h1 = relu(nd.dot(X, W1) + b1)\n",
    "    output = nd.dot(h1, W2) + b2\n",
    "    return output\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] -= lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax和交叉熵损失函数\n",
    "\n",
    "在多类Logistic回归里我们提到分开实现Softmax和交叉熵损失函数可能导致数值不稳定。这里我们直接使用Gluon提供的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "训练跟之前一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.299955, Train acc 0.888636, Test acc 0.879883\n",
      "Epoch 1. Loss: 0.295771, Train acc 0.891728, Test acc 0.888770\n",
      "Epoch 2. Loss: 0.289865, Train acc 0.892221, Test acc 0.878516\n",
      "Epoch 3. Loss: 0.282838, Train acc 0.894952, Test acc 0.888281\n",
      "Epoch 4. Loss: 0.277365, Train acc 0.897390, Test acc 0.887500\n",
      "Epoch 5. Loss: 0.273307, Train acc 0.899324, Test acc 0.890332\n",
      "Epoch 6. Loss: 0.268238, Train acc 0.900537, Test acc 0.887012\n",
      "Epoch 7. Loss: 0.262066, Train acc 0.902094, Test acc 0.890918\n",
      "Epoch 8. Loss: 0.255037, Train acc 0.904671, Test acc 0.886230\n",
      "Epoch 9. Loss: 0.252222, Train acc 0.905967, Test acc 0.888477\n"
     ]
    }
   ],
   "source": [
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0.\n",
    "    for data, label in data_iterator:\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "    return acc / len(data_iterator)\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "可以看到，加入一个隐含层后我们将精度提升了不少。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 我们使用了 `weight_scale` 来控制权重的初始化值大小，增大或者变小这个值会怎么样？\n",
    "- 尝试改变 `num_hiddens` 来控制模型的复杂度\n",
    "- 尝试加入一个新的隐含层\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/739)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
