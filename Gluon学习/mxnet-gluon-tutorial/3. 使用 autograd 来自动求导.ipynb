{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习中，我们通常使用 **梯度下降（gradient descent）** 来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。虽然梯度计算比较直观，但对于复杂的模型，例如多达数十层的神经网络，手动计算梯度非常困难。\n",
    "\n",
    "为此 MXNet 提供 `autograd` 包来自动化求导过程。虽然大部分的深度学习框架要求编译计算图来自动求导，`mxnet.autograd` 可以对正常的命令式程序进行求导，它每次在后端实时创建计算图，从而可以立即得到梯度的计算方法。\n",
    "\n",
    "下面让我们一步步介绍这个包。我们先导入`autograd`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:25:40.551987Z",
     "start_time": "2018-03-06T06:25:38.907987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet.ndarray as nd\n",
    "import mxnet.autograd as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为变量附上梯度\n",
    "\n",
    "假设我们想对函数 $f=2 x^2$ 求关于 $x$ 的导数。我们先创建变量 `x`，并赋初值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:25:45.776672Z",
     "start_time": "2018-03-06T06:25:45.770665Z"
    }
   },
   "outputs": [],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当进行求导的时候，我们需要一个地方来存 `x` 的导数，这个可以通过 NDArray 的方法 `attach_grad()` 来要求系统申请对应的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:25:52.186067Z",
     "start_time": "2018-03-06T06:25:52.182102Z"
    }
   },
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义 `f`。默认条件下，MXNet 不会自动记录和构建用于求导的计算图，我们需要使用 autograd 里的 `record()` 函数来显式的要求 MXNet 记录我们需要求导的程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:06.760682Z",
     "start_time": "2018-03-06T06:26:06.750685Z"
    }
   },
   "outputs": [],
   "source": [
    "with ag.record():\n",
    "    y = x * 2\n",
    "    z = y * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们可以通过 `z.backward()` 来进行求导。如果 `z` 不是一个标量，那么 `z.backward()` 等价于 `nd.sum(z).backward()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:10.804958Z",
     "start_time": "2018-03-06T06:26:10.778925Z"
    }
   },
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看求出来的导数是不是正确的。注意到`y = x * 2`和`z = x * y`，所以`z`等价于`2 * x * x`。它的导数那么就是 $\\frac{dz}{dx} = 4 \\times {x}$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:20.681695Z",
     "start_time": "2018-03-06T06:26:20.667690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  \n",
      "[[ 4.  8.]\n",
      " [12. 16.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1.]\n",
       " [1. 1.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x.grad: ', x.grad)\n",
    "x.grad == 4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:27.367119Z",
     "start_time": "2018-03-06T06:26:27.363123Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:30.877927Z",
     "start_time": "2018-03-06T06:26:29.612918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 2. 3.]\n",
       "<NDArray 3 @gpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 申请内存并赋值\n",
    "a = nd.array([1, 2, 3], ctx= mx.gpu(0))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:35.750801Z",
     "start_time": "2018-03-06T06:26:35.744825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[3. 4. 5.]\n",
       "<NDArray 3 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nd.array([3, 4, 5]).copyto(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:39.854732Z",
     "start_time": "2018-03-06T06:26:38.559738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 6.  8. 10.]\n",
       "<NDArray 3 @gpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对控制流求导\n",
    "\n",
    "命令式的编程的一个便利之处是几乎可以对任意的可导程序进行求导，即使里面包含了 Python 的控制流。考虑下面程序，里面包含控制流 `for` 和 `if`，但循环迭代的次数和判断语句的执行都是取决于输入的值。不同的输入会导致这个程序的执行不一样。（对于计算图框架来说，这个对应于**动态图**，就是图的结构会根据输入数据不同而改变）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:26:55.471870Z",
     "start_time": "2018-03-06T06:26:55.463911Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while nd.norm(b).asscalar() < 1000:\n",
    "        b = b * 2\n",
    "    if nd.sum(b).asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以跟之前一样使用 `record` 记录和 `backward` 求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:27:10.363229Z",
     "start_time": "2018-03-06T06:27:10.338207Z"
    }
   },
   "outputs": [],
   "source": [
    "a = nd.random_normal(shape=3)\n",
    "a.attach_grad()\n",
    "with ag.record():\n",
    "    c = f(a)\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到给定输入`a`，其输出 $\\\\f(a)= {xa}$，$x$ 的值取决于输入`a`。所以有 $\\frac{df}{da} = {x}$，我们可以很简单地评估自动求导的导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T06:27:12.415219Z",
     "start_time": "2018-03-06T06:27:12.409225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == c / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 头梯度和链式法则\n",
    "\n",
    "**注意：读者可以跳过这一小节，不会影响阅读之后的章节**\n",
    "\n",
    "当我们在一个`NDArray`上调用`backward`方法时，例如`y.backward()`，此处`y`是一个关于`x`的函数，我们将求得`y`关于`x`的导数。数学家们会把这个求导写成 $\\frac{dy(x)}{dx}$ 。还有些更复杂的情况，比如`z`是关于`y`的函数，且`y`是关于`x`的函数，我们想对`z`关于`x`求导，也就是求 $\\frac{d}{dx} z(y(x))$ 的结果。回想一下链式法则，我们可以得到$\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}$。当`y`是一个更大的`z`函数的一部分，并且我们希望求得 $\\frac{dz}{dx}$ 保存在`x.grad`中时，我们可以传入**头梯度（head gradient）** $\\frac{dz}{dy}$ 的值作为`backward()`方法的输入参数，系统会自动应用链式法则进行计算。这个参数的默认值是`nd.ones_like(y)`。关于链式法则的详细解释，请参阅[Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[40.    8.  ]\n",
      " [ 1.2   0.16]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with ag.record():\n",
    "    y = x * 2\n",
    "    z = y * x\n",
    "\n",
    "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  4.  32.]\n",
      " [108. 256.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])\n",
    "x.attach_grad()\n",
    "with ag.record():\n",
    "    y = x * x\n",
    "    z = y * x * x\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  4.  32.]\n",
      " [108. 256.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])\n",
    "x.attach_grad()\n",
    "with ag.record():\n",
    "    y = x * x\n",
    "    z = y * x * x\n",
    "head_gradient1 = nd.array([[1, 1], [1, 1]])\n",
    "z.backward(head_gradient1)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[40.   32.  ]\n",
      " [10.8   2.56]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])\n",
    "x.attach_grad()\n",
    "with ag.record():\n",
    "    y = x * x\n",
    "    z = y * x * x\n",
    "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于头梯度与链式法则，其实没有必然联系，原文放在一起讲解，反而容易让新手迷糊。[菩提树下的杨过](http://www.cnblogs.com/yjmyzz/p/7783286.html)讲解的十分详细。\n",
    "\n",
    "简单来说，头梯度就相当于提供了在前面乘的系数（对应系数相乘）。没有头梯度就当系数为 $1$.\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/744)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
